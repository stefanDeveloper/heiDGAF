from src.detector.detector import DetectorBase
import math
import numpy as np
from src.base.log_config import get_logger

module_name = "data_analysis.detector"
logger = get_logger(module_name)


class DGADetector(DetectorBase):
    """
    Detector implementation for identifying Domain Generation Algorithm (DGA) domains.

    This class extends the DetectorBase to provide specific functionality for detecting
    malicious domains generated by domain generation algorithms. It uses a machine learning
    model to analyze domain name characteristics and identify potential DGA activity.

    The detector extracts various statistical and structural features from domain names
    to make predictions about whether a domain is likely generated by a DGA.
    """

    def __init__(self, detector_config, consume_topic):
        """
        Initialize the DGA detector with configuration parameters.

        Sets up the detector with the model base URL and passes configuration to the
        base class for standard detector initialization.

        Args:
            detector_config (dict): Configuration dictionary containing detector-specific
                parameters including base_url, model, checksum, and threshold.
            consume_topic (str): Kafka topic from which the detector will consume messages.
        """
        self.model_base_url = detector_config["base_url"]
        super().__init__(detector_config, consume_topic)

    def get_model_download_url(self):
        """
        Generate the complete URL for downloading the DGA detection model.

        Constructs the URL using the base URL from configuration and appends the
        specific model filename with checksum for verification.

        Returns:
            str: Fully qualified URL where the model can be downloaded.
        """
        self.model_base_url = (
            self.model_base_url[:-1]
            if self.model_base_url[-1] == "/"
            else self.model_base_url
        )
        return f"{self.model_base_url}/files/?p=%2F{self.model}/{self.checksum}/{self.model}.pickle&dl=1"

    def get_scaler_download_url(self):
        """
        Generate the complete URL for downloading the DGA detection models scaler.

        Constructs the URL using the base URL from configuration and appends the
        specific model filename with checksum for verification.

        Returns:
            str: Fully qualified URL where the model can be downloaded.
        """
        self.model_base_url = (
            self.model_base_url[:-1]
            if self.model_base_url[-1] == "/"
            else self.model_base_url
        )
        return f"{self.model_base_url}/files/?p=%2F{self.model}/{self.checksum}/scaler.pickle&dl=1"

    def predict(self, message):
        """
        Process a message and predict if the domain is likely generated by a DGA.

        Extracts features from the domain name in the message and uses the loaded
        machine learning model to generate prediction probabilities.

        Args:
            message (dict): A dictionary containing message data, expected to have
                a "domain_name" key with the domain to analyze.

        Returns:
            np.ndarray: Prediction probabilities for each class. Typically a 2D array
                where the shape is (1, 2) for binary classification (benign/malicious).
        """
        y_pred = self.model.predict_proba(self._get_features(message["domain_name"]))
        return y_pred

    def _get_features(self, query: str):
        """Transform a dataset with new features using numpy.

        Args:
            query (str): A string to process.

        Returns:
            dict: Preprocessed data with computed features.
        """
        # Splitting by dots to calculate label length and max length
        label_parts = query.split(".")
        label_length = len(label_parts)
        label_max = max(len(part) for part in label_parts)
        label_average = len(query.strip("."))

        logger.debug("Get letter frequency")
        alc = "abcdefghijklmnopqrstuvwxyz"
        freq = np.array(
            [query.lower().count(i) / len(query) if len(query) > 0 else 0 for i in alc]
        )

        logger.debug("Get full, alpha, special, and numeric count.")

        def calculate_counts(level: str) -> np.ndarray:
            if len(level) == 0:
                return np.array([0, 0, 0, 0])

            full_count = len(level)
            alpha_count = sum(c.isalpha() for c in level) / full_count
            numeric_count = sum(c.isdigit() for c in level) / full_count
            special_count = (
                sum(not c.isalnum() and not c.isspace() for c in level) / full_count
            )

            return np.array([full_count, alpha_count, numeric_count, special_count])

        levels = {
            "fqdn": query,
            "thirdleveldomain": label_parts[0] if len(label_parts) > 2 else "",
            "secondleveldomain": label_parts[1] if len(label_parts) > 1 else "",
        }
        counts = {
            level: calculate_counts(level_value)
            for level, level_value in levels.items()
        }

        logger.debug(
            "Get standard deviation, median, variance, and mean for full, alpha, special, and numeric count."
        )
        stats = {}
        for level, count_array in counts.items():
            stats[f"{level}_std"] = np.std(count_array)
            stats[f"{level}_var"] = np.var(count_array)
            stats[f"{level}_median"] = np.median(count_array)
            stats[f"{level}_mean"] = np.mean(count_array)

        logger.debug("Start entropy calculation")

        def calculate_entropy(s: str) -> float:
            if len(s) == 0:
                return 0
            probabilities = [float(s.count(c)) / len(s) for c in dict.fromkeys(list(s))]
            entropy = -sum(p * math.log(p, 2) for p in probabilities)
            return entropy

        entropy = {level: calculate_entropy(value) for level, value in levels.items()}

        logger.debug("Finished entropy calculation")

        # Final feature aggregation as a NumPy array
        basic_features = np.array([label_length, label_max, label_average])

        # Flatten counts and stats for each level into arrays
        level_features = np.hstack([counts[level] for level in levels.keys()])

        # Entropy features
        entropy_features = np.array([entropy[level] for level in levels.keys()])

        # Concatenate all features into a single numpy array
        all_features = np.concatenate(
            [
                basic_features,
                freq,
                # freq_features,
                level_features,
                # stats_features,
                entropy_features,
            ]
        )

        logger.debug("Finished data transformation")

        return all_features.reshape(1, -1)
